{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "from tfomics import neuralnetwork as nn\n",
    "from tfomics import utils, learn\n",
    "\n",
    "# import models\n",
    "from model_zoo import fourthplace_connectomics_model\n",
    "from model_zoo import simple_connectomics_model, simple_connectomics_model2\n",
    "from model_zoo import residual_connectomics_model_PK, residual_connectomics_model, residual_connectomics_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "filename = 'processed_dataset.hdf5'\n",
    "data_path = 'D:/Dropbox/TFconnect'\n",
    "filepath = os.path.join(data_path,filename)\n",
    "\n",
    "group_name = ['processed_data']\n",
    "dataset = h5py.File(filepath,'r')\n",
    "%time dtf = np.array(dataset['/'+group_name[0]+'/dtf'])\n",
    "ltf = np.array(dataset['/'+group_name[0]+'/ltf'])\n",
    "dtf_crossval = np.array(dataset['/'+group_name[0]+'/dtf_crossval'])\n",
    "ltf_crossval = np.array(dataset['/'+group_name[0]+'/ltf_crossval'])\n",
    "\n",
    "X_train = dtf#[:10000,:,:,:]\n",
    "y_train = ltf#[:10000,:]\n",
    "X_valid = dtf_crossval#[:5000,:,:,:]\n",
    "y_valid = ltf_crossval#[:5000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fourth place competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = fourthplace_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'tfomics')\n",
    "output_name = '1d_version'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=50, \n",
    "                    patience=5, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple connectomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = simple_connectomics_model.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'tfomics')\n",
    "output_name = '1d_version'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=50, \n",
    "                    patience=5, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple connectomics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = simple_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "nnmodel.inspect_layers()\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'tf-test')\n",
    "output_name = '1d_version2'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "X2 = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': X, 'valid': X2}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=50, \n",
    "                    patience=5, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# residual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model_PK.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'tfomics')\n",
    "output_name = 'residual'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from:  ../results\\tfomics\\residual_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "nntrainer.set_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=100, num_epochs=50, \n",
    "                    patience=5, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test  loss:\t\t0.34043\n",
      "  test  accuracy:\t0.86718+/-0.00000\n",
      "  test  auc-roc:\t0.93357+/-0.00000\n",
      "  test  auc-pr:\t\t0.93072+/-0.00018\n"
     ]
    }
   ],
   "source": [
    "test = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "test_loss = nntrainer.test_model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "filename = 'D:/Dropbox/TFconnect/valideval_dataset.hdf5'\n",
    "dataset = h5py.File(filename,'r')\n",
    "group_name = ['valid_data']\n",
    "%time val_dat = np.array(dataset['/'+group_name[0]+'/vs_valid'])\n",
    "val_lbl = np.array(dataset['/'+group_name[0]+'/label_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "fragLen = 330\n",
    "N = 14\n",
    "avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "startgap = np.ceil((val_dat.shape[1] - fragLen)/N).astype('int')\n",
    "true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "\n",
    "# Counter for the \"true_lbl\" array\n",
    "cnt_ = 0\n",
    "# Counter for the \"pred_lbl\" array\n",
    "cnt_u = 0\n",
    "for a in range(val_dat.shape[0]):\n",
    "    if a%100 == 0:\n",
    "        print('\\r' + 'X'*(a//100))\n",
    "\n",
    "    # Create batch array to send thru network\n",
    "    im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "\n",
    "    # Count the number of traces in each batch\n",
    "    cnt = 0\n",
    "\n",
    "    for b in range(val_dat.shape[0]):\n",
    "\n",
    "        for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "            try:\n",
    "                im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                     val_dat[b,n:n+fragLen],\n",
    "                                     avg_F[n:n+fragLen]))\n",
    "            except:\n",
    "                from IPython.core.debugger import Tracer\n",
    "                Tracer()()\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "        # Keep track of the true labels\n",
    "        if val_lbl[a,b] == 1:\n",
    "            true_lbl[cnt_] = 1\n",
    "        else:\n",
    "            true_lbl[cnt_] = 0\n",
    "\n",
    "        cnt_ += 1\n",
    "\n",
    "    # Run batch through network\n",
    "    test = {'inputs': im_eval, 'keep_prob': 1, 'is_training': False}\n",
    "    pred_stop = nntrainer.get_activations(test, layer='output')[:,0]\n",
    "    # Average output over each group of N traces\n",
    "    for u in range(0, len(pred_stop), N):\n",
    "        pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N])\n",
    "        cnt_u += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935433871545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "wrk = auc(fpr, tpr)\n",
    "print(wrk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = dtf[0:1,:,:,:]\n",
    "test = {'inputs': X_test, 'keep_prob': 1, 'is_training': False}\n",
    "predictions = nntrainer.get_activations(test, layer='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_stop = nntrainer.get_activations(im_eval[0:1,:,:,:], layer='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# residual model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shapes\n",
    "num_data, height, width, dim = X_train.shape\n",
    "input_shape=[None, height, width, dim]\n",
    "num_labels = y_train.shape[1]  \n",
    "\n",
    "# load model\n",
    "net, placeholders, optimization = residual_connectomics_model2.model(input_shape, num_labels)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(net, placeholders)\n",
    "\n",
    "# set output file paths\n",
    "resultspath = utils.make_directory('../results', 'tf-test')\n",
    "output_name = '1d_version_residual2'\n",
    "filepath = os.path.join(resultspath, output_name)\n",
    "\n",
    "# compile neural trainer\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, optimization, save='best', filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = {'inputs': X_train, 'targets': y_train, 'keep_prob': 0.8, 'is_training': True}\n",
    "valid = {'inputs': X_valid, 'targets': y_valid, 'keep_prob': 1, 'is_training': False}\n",
    "data = {'train': train, 'valid': valid}\n",
    "learn.train_minibatch(nntrainer, data, batch_size=1000, num_epochs=50, \n",
    "                    patience=5, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nntrainer.close_sess()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
